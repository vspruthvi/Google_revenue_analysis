{
  "cells": [
    {
      "metadata": {
        "_uuid": "c9afc1260566c9b7d618da59c3e6451a54960f58"
      },
      "cell_type": "markdown",
      "source": ">80/20 rule:\nThe Pareto principle (also known as the 80/20 rule, the law of the vital few, or the principle of factor sparsity)[1][2] states that, for many events, roughly 80% of the effects come from 20% of the causes\nthe dataset given is based on the above rule"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true,
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport gc\nimport sys\nimport math\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\nfrom pandas.io.json import json_normalize\nfrom datetime import datetime\n\nimport os\nprint('done')\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "\nThe data is in CSV file format. lets view the data to understand it better."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7044a255775e59e2ce5bb2de03a74ab9f95ded0b"
      },
      "cell_type": "code",
      "source": "#data_train = pd.read_csv(\"../input/train_v2.csv\",sep=',')\ntrain_data = pd.read_csv('../input/train_v2.csv',sep=',',nrows=2000)\ndisplay(train_data.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa4f7aeecee2fde8963aef7cdf958d874bd75261"
      },
      "cell_type": "markdown",
      "source": "we can observe from the above data that four columns device, geoNetwork, totals and trafficSource are in json format.We will first flatten this data. we will be using the following function given by julian"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "562681ba84581c393f5b9d8934a70cbf3dc5e04c"
      },
      "cell_type": "code",
      "source": "def load_df(csv_path='../input/train_v2.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows,usecols=lambda col: col not in [\"hits\",\"customDimensions\"]\n)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23a8d755d290006b1662521fb278344b5d1fb0a9"
      },
      "cell_type": "markdown",
      "source": "\nnow we can easily load both the training and the testing dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f282f69e7a34fe83e1620a49f7a0999a5ba51eb"
      },
      "cell_type": "code",
      "source": "%%time\ntrain_data = load_df(\"../input/train_v2.csv\",nrows=1000000)\ntest_data = load_df(\"../input/test_v2.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a4b2db3946cf1324d054ee96ea0033a4a1cd0a52"
      },
      "cell_type": "code",
      "source": "display(train_data.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "704417d188fa3bffa36cb785a36013ca8ae62d6e"
      },
      "cell_type": "code",
      "source": "display(test_data.head(n=20))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c64d506c144527462dd78fc472292a390cf0e36"
      },
      "cell_type": "markdown",
      "source": "Before starting with analysis it is good to first observe the data and make some derivations\nI have observed the following in the above dataset:\n1. many columns have the value \"not available in demo dataset\" which will not be useful.\n2. visitId and visitStartTime is the same\n3.. date field is a combination of year, month and date"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e1b951e41cfe04c4aff392d010660d5ce8fee41"
      },
      "cell_type": "code",
      "source": "print(train_data.shape,test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "641f3dd7a0893d541828b3689ff1fefbf719bcd8"
      },
      "cell_type": "markdown",
      "source": "we can modify the date field to obtain it in proper date format, we can also make seperate columns for date, month as well as day. the following code can be used to do it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "125a1e8186ba5131e39ba1f924f07f1aad962400"
      },
      "cell_type": "code",
      "source": "def process_date(data):\n    data['date'] = data['date'].astype(str)\n    data[\"date\"] = data[\"date\"].apply(lambda x : x[:4] + \"-\" + x[4:6] + \"-\" + x[6:])\n    data[\"date\"] = pd.to_datetime(data[\"date\"])   \n    data[\"month\"] = data['date'].dt.month\n    data[\"day\"] = data['date'].dt.day\n    data[\"weekday\"] =data['date'].dt.weekday \n    return data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f395314ef949dd2000971d5ccb606ba905868237"
      },
      "cell_type": "code",
      "source": "train_data = process_date(train_data)\ntest_data = process_date(test_data)\ndisplay(train_data.head())\ndisplay(test_data.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21c568a2e26068fa44da5bbd17e7bfb9c2bc9bd1"
      },
      "cell_type": "code",
      "source": "print(test_data.shape)\nprint(train_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed7d0ca577a4199dec82316bd96ec66a8256cfa0"
      },
      "cell_type": "markdown",
      "source": "Now we will have to explore the dataset, lets first find the data type of each column"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "763afdcb3b6fecc26e1ff719c6d1c7125d72fe65"
      },
      "cell_type": "code",
      "source": "print(train_data.info())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e97da5df49e1206a82738ee6ec395425dc4882ba"
      },
      "cell_type": "markdown",
      "source": "The above result shows that we have 1 boolean type, 5 integer type 54 object type and 1 date time type of data. we have a lot of null values in our dataset, lets find a count of  all the null type of objects"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ecf641b6b19e06d82eca9e64e29888cb3cd6bb9"
      },
      "cell_type": "code",
      "source": "null_cnt = train_data.isnull().sum().sort_values()\nprint(null_cnt[null_cnt > 0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "82327b20c1f46b6d86d7e8c5498bbeb79833cb89"
      },
      "cell_type": "markdown",
      "source": "\nWe have to first fill all the null values in the dataset.\nwe have three different kinds of data\n1. object\n2. numeric\n3. boolean\none by one we have to fill the null values.\nby checking manually, i divided all the columns having null values into the above three categories.\n\nnumerical : totals.pageviews  , totals.newVisits, totals.bounces, totals.timeOnSite, totals.sessionQualityDim, totals.transactions, totals.transactionRevenue, totals.totalTransactionRevenue\n\nobject : trafficSource.keyword, trafficSource.referralPath, trafficSource.adwordsClickInfo.gclId, trafficSource.adwordsClickInfo.isVideoAd, trafficSource.adwordsClickInfo.page, trafficSource.adwordsClickInfo.slot, trafficSource.adwordsClickInfo.adNetworkType, trafficSource.adContent   \nboolean: trafficSource.isTrueDirect        \n\nnow we will deal with each of these null value columns.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "569d676444e9ad367dd26163fd113abff509abcb"
      },
      "cell_type": "code",
      "source": "# for object type column we can just fill not there in place of null\nfor column in ['trafficSource.keyword',\n            'trafficSource.referralPath',\n            'trafficSource.adwordsClickInfo.gclId',\n            'trafficSource.adwordsClickInfo.adNetworkType',\n            'trafficSource.adwordsClickInfo.isVideoAd',\n            'trafficSource.adwordsClickInfo.page',\n            'trafficSource.adwordsClickInfo.slot',\n            'trafficSource.adContent','trafficSource.isTrueDirect']:\n    train_data[column].fillna('not_there', inplace=True)\n    test_data[column].fillna('not_there', inplace=True)\n    \n    \n# for boolean type column we fill false in place of Nan\ntrain_data['trafficSource.isTrueDirect'].fillna(False, inplace=True)\ntest_data['trafficSource.isTrueDirect'].fillna(False, inplace=True)\n\n# in numeric columns we do the following changes keeping the respective columns in mind\ntrain_data['totals.pageviews'].fillna(1, inplace=True)\ntrain_data['totals.newVisits'].fillna(0, inplace=True)\ntrain_data['totals.bounces'].fillna(0, inplace=True)\ntrain_data['totals.timeOnSite'].fillna(0, inplace=True)\ntrain_data['totals.sessionQualityDim'].fillna(0, inplace=True)\ntrain_data['totals.transactions'].fillna(0, inplace=True)\ntrain_data['totals.transactionRevenue'].fillna(0, inplace=True)\ntrain_data['totals.totalTransactionRevenue'].fillna(0, inplace=True) \n\ntest_data['totals.pageviews'].fillna(1, inplace=True)\ntest_data['totals.newVisits'].fillna(0, inplace=True)\ntest_data['totals.bounces'].fillna(0, inplace=True)\ntest_data['totals.timeOnSite'].fillna(0, inplace=True)\ntest_data['totals.sessionQualityDim'].fillna(0, inplace=True)\ntest_data['totals.transactions'].fillna(0, inplace=True)\ntest_data['totals.transactionRevenue'].fillna(0, inplace=True)\ntest_data['totals.totalTransactionRevenue'].fillna(0, inplace=True)\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2aa025196ddb88f5d637bd4c685b41167075e93"
      },
      "cell_type": "markdown",
      "source": "as observed, there are few columns having the constant value \"not available in demo dataset\" we can ignore these columns. It will make predictions easier and complutationally efficient. there are few other columns with constant data type as well, these columns will not make any difference in our analysis,  so we delete these columns in the following code."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da8035cc43f7293acfa2686de942c54755a6d82a"
      },
      "cell_type": "code",
      "source": "\ndrop_column = [column for column in train_data.columns if train_data[column].nunique() == 1]\ndrop_column1 = [column for column in test_data.columns if train_data[column].nunique() == 1]\nfor c in drop_column:\n    print(c + ':', train_data[c].unique())\n\nprint('droped columns are:', drop_column)\nfinal_train=train_data.drop(drop_column, axis=1)\nfinal_test=test_data.drop(drop_column1, axis=1)\n\n\n#final_train = train_data.loc[:, (train_data != train_data.iloc[0]).any()]\n#final_test = test_data.loc[:, (test_data != test_data.iloc[0]).any()]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ab28610841aa937a86e21ae797ed5334802dfd6"
      },
      "cell_type": "markdown",
      "source": "Now if we see the size of the data sets we can observe that it is reduced."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f70cf6eecec35cd3b29d7f51326eb899e5c639d"
      },
      "cell_type": "code",
      "source": "print(final_train.shape)\nprint(final_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "137a3ec0688d7923f93b961440efb9d11eaf147b"
      },
      "cell_type": "code",
      "source": "print(final_train.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a19e80df00ba4a48632c97a5f0b8b490607baaf"
      },
      "cell_type": "markdown",
      "source": "**Now our data is ready for the analysis, we can start with the analysis and visualization.**\n"
    },
    {
      "metadata": {
        "_uuid": "197906a0da810eb7f59027e883552aa89a964fbf"
      },
      "cell_type": "markdown",
      "source": "The variable to be predicted is transaction revenue.\nso we first analysize this feature.\nAs we are predicting the natural log of sum of all transactions of the user, we code for the same and then take  a scatter plot."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "088cc530f49c81bb2588461fde00c735d91bae98"
      },
      "cell_type": "code",
      "source": "final_train[\"totals.transactionRevenue\"] = final_train[\"totals.transactionRevenue\"].astype('float')\nplot1 = final_train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(10,5))\nplt.scatter(range(plot1.shape[0]), np.sort(np.log1p(plot1[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d5dfb06833cdaacf99e07b5b3be911a31274d5a"
      },
      "cell_type": "markdown",
      "source": "Thus as per the 80/20 rule mentioned in the problem statement, very less amount of customers are contributing for the revenue being generated."
    },
    {
      "metadata": {
        "_uuid": "159731f203109535e817de5c019f6bb1b7161184"
      },
      "cell_type": "markdown",
      "source": "Nows lets analyse how the different attributes affect the revenue.\nwe first start with date."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96a7d22273f79fb3bf01226314993bb1aa3991c9"
      },
      "cell_type": "code",
      "source": "\ndef scatter_plot(var, color):\n    trace = go.Scatter(\n        x=var.index[::-1],\n        y=var.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nvar = final_train.groupby('date')['totals.transactionRevenue'].agg(['size', 'count'])\nvar.columns = [\"total count\", \"non-zero revenue count\"]\nvar = var.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(var[\"total count\"], 'grey')\ntrace2 = scatter_plot(var[\"non-zero revenue count\"], 'orange')\n\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"Date - total Count\", \"Date - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, title=\"plot of date against revenue counts\")\npy.iplot(fig, filename='date vs revenue counts')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "940264f3e61114bf590d55eee33346191d868387"
      },
      "cell_type": "markdown",
      "source": "from the above graph we can see that there is no particular pattern between the revenue generated and the dates. \nwe can notice a very high peak during end of the year 2017, except for that there are uniform peaks everywhere.\nbut there is a possibility of finding a pattern if we analyse the data weekly as there are uniform peaks ."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38fddf515c1f01cc51c6d3247841a5baeef179de"
      },
      "cell_type": "code",
      "source": "\ndef scatter_plot(var, color):\n    trace = go.Scatter(\n        x=var.index[::-1],\n        y=var.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nvar = final_train.groupby('weekday')['totals.transactionRevenue'].agg(['size', 'count'])\nvar.columns = [\"total count\", \"non-zero revenue count\"]\nvar = var.sort_index()\n#cnt_srs.index = cnt_srs.index.astype('str')\ntrace1 = scatter_plot(var[\"total count\"], 'grey')\ntrace2 = scatter_plot(var[\"non-zero revenue count\"], 'orange')\n\nfig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.08,\n                          subplot_titles=[\"weekday - total Count\", \"weekday - Non-zero Revenue count\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 2, 1)\nfig['layout'].update(height=800, width=800, title=\"plot of weekday against revenue counts\")\npy.iplot(fig, filename='weekday vs revenue counts')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6afd40556b549aa5eff0b9a0b4cfd5b213021fcb"
      },
      "cell_type": "markdown",
      "source": "From the above analysis we can see a pattern, the revenue count is high during the second and third day of the week and falls down after it during weekend.\nthe revenue count is highest on wednesdays.\n"
    },
    {
      "metadata": {
        "_uuid": "7b7e28e645d764115642a7a206d1b3fae555a043"
      },
      "cell_type": "markdown",
      "source": "We can next analyse channel grouping."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad8c6b3153ad4517b4e82ae1142319c998b5f0a9"
      },
      "cell_type": "code",
      "source": "\nchannel_group = final_train['channelGrouping'].value_counts()\nchannel_group.plot(kind='bar',rot=20, title= 'Channel Distribution ',figsize=(14,5))\nplt.show()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aeea739c54afd892eeb75b3162d0ca414870fc11"
      },
      "cell_type": "markdown",
      "source": "We can clearly see that organic search has the highest count. but we need to find out relation between channel and revenue .\nfor doing it we will group revenue by channel and see the result"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6279d3145dfd04fa7986dceb932a450c6713dd3f"
      },
      "cell_type": "code",
      "source": "train_rev = final_train[final_train['totals.transactionRevenue'] > 0].copy()\ndef nonzeroBar(a, b, colName, topN=np.nan):\n    df1=b[colName].value_counts()[:10]\n    df1.plot(kind='bar',color=\"brown\", title= colName+' for zero revenue ',figsize=(14,5))\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6f9eb71bc34a5009038dd550027ca2ded204a14c"
      },
      "cell_type": "code",
      "source": "nonzeroBar(final_train,train_rev, 'channelGrouping')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b66b7aaaf18c580c32cb1208be0b735da2b55d1"
      },
      "cell_type": "markdown",
      "source": "From the above analysis we can see that even though organic search has the highest count,but  the revenue generated is more through referral.\n"
    },
    {
      "metadata": {
        "_uuid": "b7bb0670e6f5ded7501909fa4e845ccb224771c9"
      },
      "cell_type": "markdown",
      "source": "We do the same analysis for device browser now"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6909c20879ab108c2420040f7defd03c5bebd2b7"
      },
      "cell_type": "code",
      "source": "channel_group = final_train['device.browser'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'device browser Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'device.browser')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa71dfd62a654a434dc1d70c301e9c245b23cf67"
      },
      "cell_type": "markdown",
      "source": "* We can see that chrome and safari have the highest count as well as are the two highest revenue generating browsers. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f33f01b3d24d5f0700945a289f3d7224acde545"
      },
      "cell_type": "code",
      "source": "channel_group = final_train['device.deviceCategory'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'device category Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'device.deviceCategory')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8ca09c355b17dca3b0be9da6a2ececba5523563"
      },
      "cell_type": "markdown",
      "source": "From the above graphs it is clearly seen that desktop is the highest revenue generator . \nmobile has a hight count but doesnt generate that much revenue.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67446a4bf76d7d2516b47226a889502116cf60f7"
      },
      "cell_type": "code",
      "source": "channel_group = final_train['device.operatingSystem'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'device operating System Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'device.operatingSystem')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9c487abc3751cb9e903e02a08a755e6977aefb1"
      },
      "cell_type": "code",
      "source": "\nchannel_group = final_train['geoNetwork.city'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'geo Network city Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'geoNetwork.city')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "578ce5ad2fe71bed6c76571e2b08ff8840e2beaa"
      },
      "cell_type": "markdown",
      "source": "Here mountain view has highest count but new york generates the most amount of revenue from among the columns known."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98df830b0d3d22382285a50417c1a899a06a3845"
      },
      "cell_type": "code",
      "source": "\nchannel_group = final_train['geoNetwork.region'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'geo Network region Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'geoNetwork.region')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "42d3390f28a467d326361ff94cf57a13b3b1d667"
      },
      "cell_type": "markdown",
      "source": "clearly we can see that california generates the highest revenue."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d70e2fe45a9f63d79e91bf838499062b56390443"
      },
      "cell_type": "code",
      "source": "\nchannel_group = final_train['geoNetwork.continent'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'geo Network continent Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'geoNetwork.continent')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7c1dbf1ce5bd10df760ab9b617f97d582a87084"
      },
      "cell_type": "markdown",
      "source": "Among continents america generates the highest revenue"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83be6c3e96a7fe4031067d7040f5ce194c6d8ad4"
      },
      "cell_type": "code",
      "source": "\n\nchannel_group = final_train['visitNumber'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= 'Visit Nuber Distribution ',figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'visitNumber')\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f7c9d533e9998a41bf7228177d338f652b97d31b"
      },
      "cell_type": "markdown",
      "source": "more the number of visits lesser is the probability of the person generating revenue"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91956bf89968aadaaf96db66961056c350ec4433"
      },
      "cell_type": "code",
      "source": "\n\n\nchannel_group = final_train['totals.timeOnSite'].value_counts()[:10]\nchannel_group.plot(kind='bar',rot=20, title= \"time on site Distribution\",figsize=(14,5))\nplt.show()\n\nnonzeroBar(final_train,train_rev,'totals.timeOnSite')\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "36f60780c73b9e3b77755cf7106a5caa1f33d573"
      },
      "cell_type": "markdown",
      "source": "time on site doesnt matter"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da63823e579d052dd9571253509d062023b33a90"
      },
      "cell_type": "code",
      "source": "# label encoding\ncols = [\"channelGrouping\", \"device.browser\", \"device.deviceCategory\", \"device.operatingSystem\", \"geoNetwork.city\",\n        \"geoNetwork.continent\", \"geoNetwork.country\", \"geoNetwork.metro\",\"geoNetwork.networkDomain\",\"geoNetwork.region\",\n        \"geoNetwork.subContinent\",\"trafficSource.adContent\",\"trafficSource.adwordsClickInfo.adNetworkType\", \n        \"trafficSource.adwordsClickInfo.gclId\",  \"trafficSource.adwordsClickInfo.page\", \n        \"trafficSource.adwordsClickInfo.slot\", \"trafficSource.campaign\",\"trafficSource.keyword\",\n        \"trafficSource.medium\", \"trafficSource.source\",'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.isTrueDirect']\nfor col in cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(final_train[col].values.astype('str')) + list(final_test[col].values.astype('str')))\n    final_train[col] = lbl.transform(list(final_train[col].values.astype('str')))\n    final_test[col] = lbl.transform(list(final_test[col].values.astype('str')))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6394f2814392d4e335cfd4dfe73f5985792d4627"
      },
      "cell_type": "code",
      "source": "#grouping all numerical columns and converting the m to float\ncols1 = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits','totals.timeOnSite','totals.sessionQualityDim','totals.transactions','totals.totalTransactionRevenue']    \nfor col in cols1:\n    final_train[col] = final_train[col].astype(float)\n    final_test[col] = final_test[col].astype(float)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43724b1036dde6ff5a02b71adbf26e1b603ec077"
      },
      "cell_type": "code",
      "source": "train_x = final_train[cols + cols1] \nvalid_x = final_test[cols + cols1]  \n\nfinal_train[\"totals.transactionRevenue\"] = np.log1p(final_train[\"totals.transactionRevenue\"].astype(float))\nfinal_test[\"totals.transactionRevenue\"] = np.log1p(final_test[\"totals.transactionRevenue\"].astype(float))\ntrain_y = final_train[\"totals.transactionRevenue\"]\nvalid_y=final_test[\"totals.transactionRevenue\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef8fb8626e7e6542bf6b589d01078ee35cd6130a"
      },
      "cell_type": "code",
      "source": "#lgb function\ndef func_lgb(train_x, train_y, val_x, val_y):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_x, label=train_y)\n    lgvalue = lgb.Dataset(valid_x, label=valid_y)\n    model = lgb.train(params, lgtrain,1000, valid_sets=[lgvalue], early_stopping_rounds=80, verbose_eval=100)\n    \n    predicted_val_y = model.predict(val_x, num_iteration=model.best_iteration)\n    return  model, predicted_val_y\n\n# Training the model #\nmodel, predicted_val = func_lgb(train_x, train_y, valid_x, valid_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2f09cd1777159e43855ae59c9ac33e747068204"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame({\"fullVisitorId\":final_test[\"fullVisitorId\"].values})\npredicted_val[predicted_val<0] = 0\nsubmission[\"PredictedLogRevenue\"] = np.expm1(predicted_val)\nsubmission = submission.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsubmission.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsubmission[\"PredictedLogRevenue\"] = np.log1p(submission[\"PredictedLogRevenue\"])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0f56c95c57bda41ccfda76d2a4b8dd9958f2a92"
      },
      "cell_type": "code",
      "source": "submission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "490dd94fba0748dfac7c3f01ac591dba678ed1c5"
      },
      "cell_type": "code",
      "source": "submission.to_csv(\"submit.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}